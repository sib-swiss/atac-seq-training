{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course website","text":""},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>It will also cover main steps and available tools for the bioinformatics analysis of bulk ATAC-seq data, including:</p> <p>This course will provide an introduction into ATAC-seq technology and its main applications to answer epigenetics related questions.</p> <ul> <li>Recap of the steps involved in processing and aligning the raw reads</li> <li>Quality control assessment specific to ATAC-seq</li> <li>Peak calling, annotation and visualisation</li> <li>Differential accessibility (DA) analysis</li> <li>Functional enrichment analysis of DA peaks</li> </ul> <p>Theoretical and practical sessions will be combined to provide broad context on ATAC-seq analysis methods as well as hands on experience using specific tools.</p>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes we will use lectures and exercises. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.</p>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1","title":"Day 1","text":"start end subject 9:00 10:30 Introduction to ATACseq 10:30 10:45 BREAK 10:45 12:00 Filtering and QC 12:00 13:00 LUNCH BREAK 13:00 13:45 Filtering and QC (exercises) 13:45 15:00 Peak Calling 15:00 15:15 BREAK 15:15 15:45 Peak Calling (exercises) 15:45 17:00 Visualisation"},{"location":"course_schedule/#day-2","title":"Day 2","text":"start end subject 9:00 9:15 Recap of yesterday 09:15 10:30 Peaks Coverage (exercises) 10:30 10:45 BREAK 10:45 11:30 Differential Accessibility 11:30 12:00 Differential Accessibility (exercises)  12:00 13:00 LUNCH BREAK 13:30 15:00 Peaks Annotation  15:00 15:15 BREAK 15:15 17:00 Peaks Annotation (exercises)"},{"location":"exercises/","title":"Exercises","text":""},{"location":"exercises/#material","title":"Material","text":"<p> Download the presentation</p> <ul> <li>Mkdocs Website</li> <li>Mkdocs material website</li> <li>Course website template on github</li> </ul>"},{"location":"exercises/#forking-and-cloning-the-template","title":"Forking and cloning the template","text":"<p>Go to https://github.com/sib-swiss/course_website_template, and click on Use this template:</p> <p>Choose the namespace in which you want to use the website template, choose a name, and initiate the new repository by finalising with Create repository from template:</p> <p>Now you can find the new repository at <code>https://github.com/[NAMESPACE]/[REPONAME]</code>. In order to clone the repository to a local directory, click on Code and copy the github address that you can use for cloning to your clipboard:</p> <p>After that, you open a terminal (e.g. Windows Powershell or your favourite terminal) <code>cd</code> to a directory you want to clone your repository in (e.g. to <code>C:\\Users\\myname\\Documents</code>) and type:</p> <pre><code>git clone https://github.com/[NAMESPACE]/[REPONAME].git # the last part can be pasted from github\n</code></pre>"},{"location":"exercises/#serving-a-website-locally","title":"Serving a website locally","text":"<p>In order to work on your website, it is convenient if you can serve it locally and directly see the effects of your work. In order to do that use the terminal to go into the repository directory, (e.g. <code>C:\\Users\\myname\\Documents\\reponame</code>) and type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Now type <code>http://localhost:8000</code> in your favourite browser, and your website should be visible.</p> <p>Note</p> <p>The website already contains content. Of course, it is up to you whether you want to keep it. In any way, you can use it as an example on how to use markdown. </p> <p>Open the file <code>index.md</code> from the directory <code>docs</code> in your favourite text editor. Add some text to the page (e.g. <code>hello world!</code>) and save the file. See whether your changes are passed to the locally served website.</p> <p>Stopping <code>mkdocs serve</code></p> <p>After you have finished working on your website you will have to stop the serving process. Otherwise, it will continue in the background and keep port 8000 (and CPU) occupied. Stop the serving process with Ctrl+C .</p>"},{"location":"exercises/#the-file-structure","title":"The file structure","text":"<p>The total file structure of the template looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 LICENCE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2514\u2500\u2500 images\n\u2502   \u2502       \u251c\u2500\u2500 SIB_logo.svg\n\u2502   \u2502       \u251c\u2500\u2500 reactions_zoom.png\n\u2502   \u2502       \u251c\u2500\u2500 reply_in_thread.png\n\u2502   \u2502       \u2514\u2500\u2500 zoom_icons.png\n\u2502   \u251c\u2500\u2500 course_schedule.md\n\u2502   \u251c\u2500\u2500 exercises.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 precourse.md\n\u2502   \u2514\u2500\u2500 stylesheets\n\u2502       \u2514\u2500\u2500 extra.css\n\u2514\u2500\u2500 mkdocs.yml\n\n4 directories, 12 files\n</code></pre> <p>The main directory contains:</p> <ul> <li><code>LICENCE</code>: a licence file (in this case cc-by-4.0)</li> <li><code>README.md</code>: the readme displayed at the github repository</li> <li><code>docs</code>: a directory with all website content, including:<ul> <li><code>assets</code>: a directory everything that is not directly rendered (e.g. images, pdfs)</li> <li>files ending with <code>*.md</code>: the actual markdown files that are rendered into html</li> <li><code>stylesheets</code>: a directory with <code>.css</code> file(s) for defining the website style format</li> </ul> </li> <li><code>mkdocs.yml</code>: a YAML file that is used by <code>mkdocs</code> in which you specify:<ul> <li>Website structure</li> <li>Meta information</li> <li>Plugins</li> </ul> </li> </ul>"},{"location":"exercises/#setting-up-the-website-infrastructure","title":"Setting up the website infrastructure","text":"<p>Open <code>mkdocs.yml</code> in your favourite text editor. Have a look at the first part:</p> <pre><code>site_name: Course template\n\nnav:\n    - Home: index.md\n    - Precourse preparations: precourse.md\n    - Course schedule: course_schedule.md\n    - Exercises: exercises.md\n</code></pre> <p>The first line (<code>site_name</code>) let\u2019s you change website name. Change it to something that makes sense to you, and check whether it has changed in the locally hosted site.</p> <p>With the part named <code>nav</code>, you can change the website structure and with that navigation. The file <code>index.md</code> should always be there, this is the \u2018homepage\u2019.</p> <p>Now we will generate a new page that is a subchapter of Exercises. In order to do so, follow the following steps:</p> <ul> <li>Generate a directory within the directory <code>docs</code> called <code>exercises</code></li> <li>Within the <code>exercises</code> directory generate a new file called <code>exercises_day1.md</code></li> <li>Adjust the <code>nav</code> part of <code>mkdocs.yml</code> like so:</li> </ul> <pre><code>nav:\n    - Home: index.md\n    - Precourse preparations: precourse.md\n    - Course schedule: course_schedule.md\n    - Exercises:\n      - Day 1: exercises/exercises_day1.md\n</code></pre> <p>Now a new collapsible menu will appear, containing your new page.</p>"},{"location":"exercises/#referring-to-the-right-repo","title":"Referring to the right repo","text":"<p>In <code>mkdocs.yml</code> have a look at the repository part:</p> <pre><code># Repository\nrepo_name: sib-swiss/course_website_template\nrepo_url: https://github.com/sib-swiss/course_website_template\n</code></pre> <p>The course website is now hosted at your own repository. Therefore, change the repository name and url according to your own.</p>"},{"location":"exercises/#markdown-syntax","title":"Markdown syntax","text":"<p>You can use general github markdown syntax in order to generate a formatted html page. Have a look here.</p> <p>Now, convert the rendered text below into markdown. Add your markdown text to the file <code>exercises_day1.md</code> and see whether you get the expected result while you type.</p> Rendered markdown Answer <pre><code>### My markdown exercise\n\nWith plain markdown you can highlight in two ways:\n\n1. *Italic*\n2. **Bold**\n\nYou can add a link to your favourite [website](https://www.sib.swiss/).\nOr add an image from that website (find it at `https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg`):\n\n![](https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg)\n\nYou can also add a local image (this one is stored in `../assets/images/zoom_icons.png`):\n\n![](../assets/images/zoom_icons.png)\n\nSharing a code is easy, inline you refer to code like this: `pip install mkdocs`.\nBut often it's more convenient in a code block, e.g. with shell highlighting:\n\n```sh\nFILE=my_genes.csv\ncat $FILE | cut -f 1,2 -d ','\n```\n\nOr with R highlighting for example:\n\n```r\ndf &lt;- read.csv('my_genes.csv')\n```\n</code></pre>"},{"location":"exercises/#my-markdown-exercise","title":"My markdown exercise","text":"<p>With plain markdown you can highlight in two ways:</p> <ol> <li>Italic</li> <li>Bold</li> </ol> <p>You can add a link to your favourite website. Or add an image from that website (find it at <code>https://www.sib.swiss/images/banners/banner_research_infrastructure.jpg</code>):</p> <p></p> <p>You can also add a local image (this one is stored in <code>../assets/images/zoom_icons.png</code>):</p> <p></p> <p>Sharing a code is easy, inline you refer to code like this: <code>pip install mkdocs</code>. But often it\u2019s more convenient in a code block, e.g. with shell highlighting:</p> <pre><code>FILE=my_genes.csv\ncat $FILE | cut -f 1,2 -d ','\n</code></pre> <p>Or with R highlighting for example:</p> <pre><code>df &lt;- read.csv('my_genes.csv')\n</code></pre>"},{"location":"exercises/#additional-features-of-mkdocs-material","title":"Additional features of Mkdocs material","text":"<p>Some additional features are very convenient for generating a website for teaching. For example admonitions:</p> code <pre><code>!!! warning\n    Do not overcommit the server!\n</code></pre> output <p>Warning</p> <p>Do not overcommit the server!</p> <p>Also very convenient can be content tabs:</p> <p>code:</p> <pre><code>=== \"R\"\n    Generating a vector of integers:\n    ```r\n    a &lt;- c(5,4,3,2,1)\n    ```\n=== \"python\"\n    Generating a list of integers:\n    ```python\n    a = [5,4,3,2,1]\n    ```\n</code></pre> <p>output:</p> R <p>Generating a vector of integers: <pre><code>a &lt;- c(5,4,3,2,1)\n</code></pre></p> python <p>Generating a list of integers: <pre><code>a = [5,4,3,2,1]\n</code></pre></p> <p>Mkdocs material comes with a very wide range of emoticons and icons. Use the search field in the link to search for icons. Here\u2019s an example:</p> code <pre><code>Write an e-mail :material-send:, add a pdf :material-file-pdf: and wait :clock1:\n</code></pre> output <p>Write an e-mail , add a pdf :material-file-pdf: and wait </p> <p>You can make a button like this:</p> code <pre><code>[Download the presentation](../assets/pdf/introduction_gh_pages.pdf){: .md-button }\n</code></pre> output <p>Download the presentation</p> <p>You can also add an icon to a button:</p> code <pre><code>[:fontawesome-solid-file-pdf: Download the presentation](../assets/pdf/introduction_gh_pages.pdf){: .md-button }\n</code></pre> output <p> Download the presentation</p> <p>Lastly, you can incorporate <code>html</code>. This can particularly be convenient if you want to control the size of images.</p> code <pre><code>&lt;figure&gt;\n  &lt;img src=\"../assets/images/zoom_icons.png\" width=\"300\"/&gt;\n&lt;/figure&gt;\n\n&lt;figure&gt;\n  &lt;img src=\"../assets/images/zoom_icons.png\" width=\"100\"/&gt;\n&lt;/figure&gt;\n</code></pre> output <p> </p> <p> </p>"},{"location":"exercises/#byo-workshop","title":"BYO workshop","text":"<p>If you have brought your own course material, now you can start with generating a page containing your own course material.</p>"},{"location":"exercises/#host-the-website-at-githubio","title":"Host the website at github.io","text":"<p>You can deploy your website as a github page by running the command:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>It will become available at <code>[NAMESPACE].github.io/[REPONAME]</code>. This can take more than an hour if you are deploying for the first time. The next time you update your website, it will usually take less then a minute.</p>"},{"location":"exercises/#pushing-to-a-remote-repository","title":"Pushing to a remote repository","text":"<p>The website html is created in a directory called <code>site</code> inside your repository directory. This directory is used to locally host the website, but usually you don\u2019t want to push it to your master branch. Therefore add it to <code>.gitignore</code>:</p> <pre><code>echo \"site\" &gt;&gt; .gitignore\n</code></pre> <p>Note</p> <p>Anything that is added to the file <code>.gitignore</code> is not added to the git repository. You have to add such files/directories only once. You can of course also open <code>.gitignore</code> in your favourite text editor and modify it in there.</p> <p>Now, you can add your changes to make a commit:</p> <pre><code>git add --all\n</code></pre> <p>And commit your changes to your local repository like this:</p> <pre><code>git commit -m 'short description'\n</code></pre> <p>And finally push it to the remote:</p> <pre><code>git push\n</code></pre>"},{"location":"precourse/","title":"Precourse preparations","text":""},{"location":"precourse/#knowladge","title":"Knowladge","text":""},{"location":"precourse/#ngs","title":"NGS","text":"<p>As announced in the course registration webpage, we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques.</p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>Practical knowledge of the UNIX command line is also required to be able to follow this course.</p>"},{"location":"precourse/#r","title":"R","text":"<p>A basic knowledge of the R language is required to perform most analytical steps the downstream analysis.</p>"},{"location":"precourse/#technical","title":"Technical","text":"<p>Attendees should have a Wi-Fi enabled computer and the Integrative Genomics Viewer (IGV) installed.</p> <p>An online R and RStudio environment will be provided. In order to access that environment your computer needs to be able to access http websites (not https). You can check this by browsing this website.</p>"},{"location":"days/00_connect_to_server/","title":"Connect to RStudio server","text":"<p>If you are enrolled in the course, you have access to RStudio server. You can find a link to it and your credentials to connect in the Google Docs that has been shared with you.  </p> <p>During the first part of the course we will be using bash commands and executing different software. For that, we will work on the built-in terminal of RStudio.   </p> <p>To access the terminal, select the tab Terminal on the top left of the panel:</p> <p></p> <p>Check your current directory, you should be in your home directory <code>/home/rstudio</code> </p> <pre><code>pwd\n</code></pre> <p>In order to activate your conda enviroment and have access to different tools necessary for this course, please run the following command in your terminal:</p> <pre><code>conda init\nsource /home/rstudio/.bashrc\nconda activate atac_env\n</code></pre> <p>You should now be able to run tools like samtools, bedtools, macs2, etc. Check with: <pre><code>bedtools --version\n</code></pre></p> <p>For the second part of the course we will use only R commands. For that, we will move back to RStudio console, by selecting the tab Console.</p>"},{"location":"days/01_filter_bam_files/","title":"Post-Alignment Reads Filtering","text":""},{"location":"days/01_filter_bam_files/#overview","title":"Overview","text":"<p>In this section, we will filter BAM files to remove low-quality reads and prepare them for downstream analysis.</p>"},{"location":"days/01_filter_bam_files/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Apply quality filters to BAM files using samtools</li> <li>Remove reads from blacklisted genomic regions</li> <li>Understand ATAC-seq specific filtering considerations</li> </ul>"},{"location":"days/01_filter_bam_files/#0-dataset","title":"0. Dataset","text":"<p>We are going to work with a subset of the publicly available ATAC-seq dataset from Liu et al. 2019. We will process and compare data from 2 different adult mouse tissues: </p> <ul> <li>Kidney: Rep1, Rep2     </li> <li>Cerebrum: Rep1, Rep2  </li> </ul> <p>In order to save time, raw reads have already been trimmed using Trim Galore and mapped to the reference genome (mm10) using bowtie2 in end-to-end mode. Therefore, we will start the analysis directly from the alignment output (.bam files).  </p> <p>To avoid large waiting times during high-demanding computational steps, .bam files have been subset to keep only reads aligning to chromosome 6.  </p> <p>Note</p> <p>You can find the .bam files in <code>/data/Liu_alignments_chr6/</code> folder. Raw data can be found in NCBI Sequence Read Archive.</p>"},{"location":"days/01_filter_bam_files/#1-filtering-reads-from-alignments","title":"1. Filtering Reads from Alignments","text":""},{"location":"days/01_filter_bam_files/#11-common-ngs-filtering-steps","title":"1.1 Common NGS Filtering Steps","text":"<p>To ensure high-quality data, it\u2019s important to filter aligned reads based on standard NGS criteria, such as:</p> <ul> <li>Removing duplicate reads (often PCR artifacts).  </li> <li>Applying minimum mapping quality thresholds.  </li> </ul> <p>These filters help reduce noise and improve the accuracy of downstream analyses.</p> <p>Note</p> <p>Tools like Picard can identify and mark duplicate reads. In this dataset, duplicates have already been marked in the provided .bam files. To learn more about Picard have a look here.   </p> <p>A useful tool for filtering alignments is <code>samtools view</code>. It allows printing all the alignments from a .bam file in SAM format, with the option to previously filter them based on specific flags (e.g. mapped/unmapped, primary/secondary, duplicates).  </p> <p>In the next exercises, you will use samtools view to visualise the alignment output and filter .bam files according to various criteria.</p> <p>Task 1: Visualise alignment output </p> <ul> <li>Visualise one of the .bam files with samtools running the following command:</li> </ul> <p><pre><code>samtools view /data/Liu_alignments_chr6/Kidney_rep1.bam | head\n</code></pre> Can you recognise what the different columns represent? Why the 3rd column contains only number 6? What is the 2nd column? </p> Answer <p>The columns correspond to SAM format. You can find an explanation of each column here. The 3rd column represents the chromosome name. In this tutorial, .bam files have been subset to contain only chr6, thus it makes sense they are all the same. The 2nd column contains the FLAG, you can use this value to filter out or in certain reads.  </p> <p>Task 2: Quality filtering with samtools </p> <p>Check in this website, which FLAG number  would you need in order to filter out from the alignments:  </p> <ul> <li>Unmapped reads</li> <li>Reads who\u2019s mate is unmapped</li> <li>Read duplicates</li> <li>Not primary alignment reads</li> </ul> Answer <p>You need flag: 1292</p> <p>Now, use samtools view manual to filter the previous .bam file based on all these criteria:  </p> <ul> <li>Filter out reads with flag 1292.  </li> <li>Keep only paired reads.  </li> <li>Filter out reads with mapping quality &lt; 10.  </li> </ul> Hint You can use:        -f and -F flags to filter in or out reads (respectively) based on a combination of SAM Flags; use -q: for MapQ threshold  Solution <pre><code>samtools view  -q 10 -f 1 -F 1292 /data/Liu_alignments_chr6/Kidney_rep2.bam | head\n</code></pre> <p>Finally, apply these filters to all .bam files and save them in .bam format (check parameter -b for that) in <code>results/01_filtered_bams/&lt;sample_name&gt;.qc_filt.bam</code></p> Solution <pre><code># To be able to iterate through all samples:\nsamples=(Kidney_rep1 Kidney_rep2 Cerebrum_rep1 Cerebrum_rep2)\n\n# create new directory for filtered bams\nmkdir -p results/01_filtered_bams\n\n# save the path to the folder in a variable\npath_bams=\"results/01_filtered_bams\"\n\n\n# filter files using a for loop\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    samtools view -h -b -q 10 -f 1 -F 1292 -o $path_bams/$sample_name.qc_filt.bam /data/Liu_alignments_chr6/$sample_name.bam\ndone\n</code></pre> <ul> <li>-h: keep header  </li> <li>-b: output in bam format  </li> <li>-q 10: minimum mapping quality 10  </li> <li>-f 1: keep paired  </li> <li>-F 1292: exclude reads with any of the following flags: read unmapped, mate unmapped, not primary alignment, read is duplicate</li> </ul> <p>After filtering we will sort and index the bam files for the next step</p> <p>Task 3: Sort and index BAM files </p> <p>Next, sort and index the bam files for downstream analysis.</p> <pre><code>for sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run commands\n    samtools sort -o $path_bams/$sample_name.qc_filt.sorted.bam $path_bams/$sample_name.qc_filt.bam\n    samtools index $path_bams/$sample_name.qc_filt.sorted.bam\ndone\n</code></pre> <p>To avoid confusion and large size files, keep only the sorted and indexed bams:</p> <p>Warning</p> <p>Important before deleting: Run this command only if you have named your files exactly as specified in this tutorial and have completed all sorting and indexing steps from the previous command.</p> <pre><code>rm results/01_filtered_bams/*qc_filt.bam\n</code></pre>"},{"location":"days/01_filter_bam_files/#12-atacseq-related-filtering-steps","title":"1.2 ATACseq related filtering steps","text":"<p>Mitochondria DNA is nucleosome free, therefore it is more accessible for Tn5 and several reads may have originated from mitochondrial DNA. After having assessed mitochondrial % on the QC, we can discard reads coming from chmMT to avoid biases in downstream analysis.</p> <p>Since we are working only with chr6 we don\u2019t need to do this step, but here is the command you could use for that:</p> <pre><code>samtools view -h input.bam | awk  '($3 != \"MT\")' | samtools view -hb - &gt; output.bam\n</code></pre> <p>Note</p> <p>The mitochondrial chromosome name may differ depending on the reference genome (e.g., \u201cMT\u201d, \u201cchrM\u201d, \u201cchrMT\u201d).</p> <p>Next, we will remove reads overlapping problematic regions of the genome. ENCODE consortium has created comprehensive lists of such regions (anomalous, unstructured or high signal in NGS experiments) for different genome species (including mouse mm10). These lists are called ENCODE Blacklists, and you can find them here. </p> <p>Note</p> <p>The regions for mm10 have been dowloaded as a .bed file, you can find it here: <code>/data/references/mm10-blacklist.v2.nochr.bed</code></p> <p>Task 4: Remove blacklist regions</p> <ul> <li>Using <code>bedtools intersect</code> filter out reads overlapping regions in the Blacklist .bed file</li> <li>Use previously filtered .bam files as input: <code>results/01_filtered_bams/*qc_filt.sorted.bam</code>, don\u2019t forget to specify your input is in BAM format</li> <li>Use the <code>/data/references/mm10-blacklist.v2.nochr.bed</code> as regions to filter out reads from (it is already sorted)</li> <li>Save the results in <code>results/01_filtered_bams/</code> in BAM format with the following output name: <code>&lt;sample_name&gt;.qc_bl_filt.bam</code> </li> </ul> <p>Bedtools</p> <p><code>Bedtools intersect</code> allows one to screen for overlaps between two sets of genomic features/regions, and then decide on which kind of information do you want to report. Here, we will intersect: a) Aligned reads to the genome (filtered and sorted .bam files) b) Problematic regions listed in Blacklist .bed file We do not want to keep the reads that overlap Blacklist regions. You can find documentation on which parameters to use here</p> Solution <pre><code>blacklist=\"/data/references/mm10-blacklist.v2.nochr.bed\"\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    bedtools intersect -v -abam $path_bams/$sample_name.qc_filt.sorted.bam -b $blacklist &gt; $path_bams/$sample_name.qc_bl_filt.bam\ndone\n</code></pre> <p>Parameter explanation: -v: only report those entries in A that have no overlap with B -abam: input is in bam format </p> <p>Task 5: Sort and index the previous files </p> <p>Sort and index the bam files for downstream analysis.  </p> <pre><code>for sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run commands\n    samtools sort -o $path_bams/$sample_name.qc_bl_filt.sorted.bam $path_bams/$sample_name.qc_bl_filt.bam\n    samtools index $path_bams/$sample_name.qc_bl_filt.sorted.bam\ndone\n</code></pre> <p>After sorting the .bam files, we don\u2019t need the unsorted .bam. To free some space we will remove the unsorted bam files (intermediate files) .</p> <p>Warning</p> <p>Important before deleting: Run this command only if you have named your files exactly as specified in this tutorial and have completed all sorting and indexing steps from the previous command.</p> <pre><code>rm results/01_filtered_bams/*qc_bl_filt.bam\n</code></pre> <p>Bonus Task</p> <p>Taks 6: Convert .bam file into .bigwig format</p> <p>In order to visualise ATAC-seq coverage, you can convert .bam files into .bigwig format, used to represent coverage tracks in genome browsers like IGV. A tool that allows you to do this conversion is <code>deeptools bamCoverage</code></p> <p>Next, convert the sorted and filterd .bams into .bigwigs</p> <pre><code>mkdir results/01_filtered_bams/filt_bigwigs\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    #run command\n    bamCoverage -b $path_bams/$sample_name.qc_bl_filt.sorted.bam -o $path_bams/filt_bigwigs/$sample_name.bw --region 6:1500000:20000000 --normalizeUsing CPM\ndone\n</code></pre> <p>Download the .bigwigs and load them into IGV.  Remember to use mm10 genome.  </p> <p>Look into Asns gene, which tissue has higher DNA accessibility? </p>"},{"location":"days/02_QC_post_alignment/","title":"Quality Control Post-Alignment","text":""},{"location":"days/02_QC_post_alignment/#overview","title":"Overview","text":"<p>After filtering our BAM files, we need to assess the quality of our ATAC-seq experiment using specialized metrics.</p>"},{"location":"days/02_QC_post_alignment/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand key ATAC-seq quality metrics</li> <li>Use ATAQV to generate comprehensive QC reports</li> <li>Interpret QC results to assess experiment quality</li> </ul> <p>In order to assess the quality of the ATAC-seq experiment, there are several metrics that can be calculated from the aligned reads.</p>"},{"location":"days/02_QC_post_alignment/#1-key-atac-seq-quality-metrics","title":"1. Key ATAC-seq Quality Metrics","text":"<p>Fragment size distribution: The distribution of fragment sizes can indicate the quality of the library preparation. A good ATAC-seq library should have a characteristic pattern with peaks corresponding to nucleosome-free regions (NFRs) and mono-, di-, and tri-nucleosomes.</p> <p>TSS enrichment: The enrichment of reads at transcription start sites (TSS) is a key indicator of data quality. High-quality ATAC-seq data should show a strong enrichment of reads at TSSs.</p>"},{"location":"days/02_QC_post_alignment/#2-ataqv-tool","title":"2. ATAQV Tool","text":"<p><code>ATAQV</code> is a specialized tool that calculates a variety of QC metrics for ATAC-seq data. It provides a comprehensive report that includes fragment size distribution, TSS enrichment, and other important metrics.</p> <p>Task 1: Generate individual QC reports with ATAQV</p> <ul> <li>Create a new folder for QC results: <code>results/02_QC_post_alignment</code> </li> <li>Run ATAQV on each filtered BAM file to generate individual QC metrics   </li> <li>Each sample will produce its own detailed QC report </li> </ul> <p>Ataqv</p> <p>You can find further information about ATAQC tool and commands in the Usage section here. Ataqv needs a Transcription Start Site (TSS) reference file to compute TSS enrichment score. You will find this file in: <code>/data/references/ENCODE_mm10_M21_TSS_reference.bed</code>. Source: TSS reference downloaded from ENCODE Project</p> Hint <p>Run <code>ataqv</code> for each <code>*qc_bl_filt.sorted.bam</code> file</p> <p>Key parameters needed: - <code>--tss-file</code>: Path to TSS reference file - <code>--metrics-file</code>: Output JSON file path - <code>--name</code>: Sample name - <code>mouse</code>: Genome reference</p> Solution <p><pre><code>mkdir -p results/02_QC_post_alignment\nTSS_bed=\"/data/references/ENCODE_mm10_M21_TSS_reference.bed\"\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    ataqv --name $sample_name --metrics-file results/02_QC_post_alignment/$sample_name.ataqv.json --tss-file $TSS_bed mouse $path_bams/$sample_name.qc_bl_filt.sorted.bam &gt; results/02_QC_post_alignment/$sample_name.ataqv.out\ndone\n</code></pre> Parameter explanations: - <code>--name</code>: Sample name for output files - <code>--metrics-file</code>: Output file for metrics in JSON format - <code>--tss-file</code>: BED file with TSS locations - <code>mouse</code>: Genome reference (mouse or human) - <code>$bam</code>: Input BAM file</p> <p>Task 2: Create multi-sample summary report</p> <p>Compile all individual QC reports into a comprehensive summary. Use the <code>mkarv</code> tool from ATAQV to combine all JSON files into an interactive HTML report</p> Hint <p>Use <code>mkarv</code> command to process all <code>*.json</code> files</p> <ul> <li>Specify output directory name for the HTML report</li> <li>Include all JSON files with wildcard pattern</li> </ul> Solution <pre><code># Create summary report from all JSON files without changing directories\nmkarv results/02_QC_post_alignment/summary_ataqv results/02_QC_post_alignment/*.ataqv.json\n</code></pre> <p>What this does: - <code>mkarv</code>: Tool to create interactive HTML summary report. - <code>results/02_QC_post_alignment/summary_ataqv</code>: Output directory for the HTML report. - <code>results/02_QC_post_alignment/*.ataqv.json</code>: Include all JSON files using full path.  </p>"},{"location":"days/02_QC_post_alignment/#3-interpret-qc-results","title":"3. Interpret QC Results","text":"<p>Open the QC report located at: <code>results/02_QC_post_alignment/summary_ataqv/index.html</code></p> <p>Task 3: Assess the quality of ATAC-seq experiment</p> <ol> <li>Overall Assessment: Do you think the experiment worked well? </li> <li>Fragment Size Distribution: Do you see the expected nucleosome pattern?</li> <li>TSS Enrichment: Are the TSS enrichment scores acceptable?</li> <li>Concerning Metrics: Are there any metrics that would concern you?</li> </ol>"},{"location":"days/03_peak_calling/","title":"Peak Calling","text":""},{"location":"days/03_peak_calling/#overview","title":"Overview","text":"<p>After quality control, we will identify accessible chromatin regions (peaks) using different approaches and fragment size filters.  </p> <p>We will compare peak calling results using: 1. Only nucleosome-free fragments 2. All fragments combined 3. HMMRATAC (ATAC-seq specific peak caller)  </p>"},{"location":"days/03_peak_calling/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand nucleosome-free vs. nucleosome-associated fragments</li> <li>Use MACS3 for peak calling with different fragment size filters</li> <li>Compare peak calling results from different approaches</li> <li>Learn about ATAC-seq specific peak caller HMMRATAC  </li> </ul>"},{"location":"days/03_peak_calling/#1-fragment-size-filtering","title":"1. Fragment size filtering","text":"<p>Before calling peaks with MACS3, we will separate fragments based on their size:</p> <ul> <li>Nucleosome-free (NF) fragments (&lt; 100 bp): Represent open chromatin regions</li> <li>Nucleosome-associated fragments (&gt; 100 bp): Represent regions with positioned nucleosomes</li> </ul> <p>Task 1: Filter for nucleosome-free (NF) fragments</p> <p>We can do that using <code>samtools view</code> as we have seen in previous exercises, and filter reads based on fragment length.  </p> <ul> <li>Which column from the .bam file contains fragment length information?  </li> </ul> Answer <p>In SAM format, column 9 is described as \u201cTLEN: signed observed Template LENgth\u201d, which corresponds to the insert size length. </p> <p>With the following code, you will: - Create output directory: <code>results/01_filtered_bams/NF_bams</code>. - Use <code>samtool view</code> to filter BAM files to keep only fragments with length 1-100 bp.   - Save filtered files as: <code>results/01_filtered_bams/NF_bams/${sample_name}_NF.bam</code>. - Maintain BAM file headers during filtering.  </p> <p>Code</p> <pre><code># create new directory for peaks\nmkdir -p results/01_filtered_bams/NF_bams\n\n# filter for NF fragments only\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    samtools view -h $path_bams/$sample_name.qc_bl_filt.sorted.bam | awk 'substr($0,1,1)==\"@\" || ($9&gt;= 1 &amp;&amp; $9&lt;=100) || ($9&lt;=-1 &amp;&amp; $9&gt;=-100)' | \\\n    samtools view -b &gt; $path_bams/NF_bams/${sample_name}_NF.bam \ndone \n</code></pre> <p>Parameter explanations: - <code>-h</code>: Keep header in output - <code>-b</code>: Output in BAM format - <code>$9</code>: Insert size (TLEN field in SAM format) - Filter logic: Keep fragments with insert size between 1 and 100bp (nucleosome-free regions)</p> <p>Task 2: sort and index BAM files</p> <ul> <li>Sort and index the bams files for next step</li> <li>Remove the unsorted file</li> </ul> <pre><code>for sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    samtools sort -o $path_bams/NF_bams/${sample_name}_NF.sorted.bam $path_bams/NF_bams/${sample_name}_NF.bam\n    samtools index $path_bams/NF_bams/${sample_name}_NF.sorted.bam\ndone\n</code></pre> <p>Warning</p> <p>Important before deleting: Run this command only if you have named your files exactly as specified in this tutorial and have completed all sorting and indexing steps from the previous command.</p> <p>Remove intermediate files <pre><code>rm results/01_filtered_bams/NF_bams/*_NF.bam\n</code></pre></p>"},{"location":"days/03_peak_calling/#2-peak-calling-strategies","title":"2. Peak calling strategies","text":""},{"location":"days/03_peak_calling/#21-macs3-peak-calling","title":"2.1 MACS3 Peak Calling","text":"<p>MACS3 is a widely used peak calling tool that can handle both narrow and broad peaks. We\u2019ll start using MACS3 function \u201ccallpeak\u201d and compare results using different fragment size filters.</p>"},{"location":"days/03_peak_calling/#peak-calling-with-nucleosome-free-fragments","title":"Peak Calling with Nucleosome-Free Fragments","text":"<p>We will first call peaks using MACS3 on NF reads only, focusing on fragments most likely to represent open chromatin regions.</p> <p>Task 3: MACS3 on NF fragments</p> <ul> <li>Create a new folder named: <code>results/03_peak_calling</code></li> <li>Create a subfolder inside called: <code>NF_peaks</code></li> <li>Call peaks on NF reads using MACS3 callpeak function. Save the results inside <code>results/03_peak_calling/NF_peaks/</code> and name the files as: <code>NF_peaks_${sample_name}</code></li> </ul> <p>Macs3</p> <p>You can have a look at the MACS3 documentation for more details on the parameters used here</p> Hint <p>For ATAC-seq data, we will use the BAMPE format, which is suitable for paired-end data and we will set the genome size to \u201cmm\u201d for mouse. We will also set a q-value cutoff of 0.01 to control the false discovery rate.</p> Solution <p><pre><code>mkdir -p results/03_peak_calling/NF_peaks\npath_peaks=\"results/03_peak_calling/\"\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    macs3 callpeak -f BAMPE -t $bams_path/${sample_name}_NF.sorted.bam -g mm -q 0.01 --name ${sample_name}_NF --outdir results/03_peak_calling/NF_peaks/NF_peaks_${sample_name}/\ndone\n</code></pre> parameters explanation: <code>-f BAMPE</code>: input file format is BAM paired-end <code>-t</code>: input file (BAM file) <code>-g mm</code>: It\u2019s the mappable genome size or effective genome size (some are pre-computed, like mouse, and you can specify \u201cmm\u201d <code>-q 0.01</code>: q-value cutoff for peak detection <code>--outdir</code>: output directory for peak files  </p>"},{"location":"days/03_peak_calling/#peak-calling-with-all-fragments","title":"Peak Calling with all Fragments","text":"<p>We will do the same, but using all fragments </p> <p>Task 4: MACS3 on all fragments</p> <ul> <li>Create a new folder named: <code>results/03_peak_calling/all_peaks</code></li> <li>Call peaks on all filtered reads using MACS3 callpeak function. Save the results inside <code>results/03_peak_calling/all_peaks/</code> and name the files as: <code>all_peaks_${sample_name}</code></li> </ul> Solution <pre><code>mkdir -p results/03_peak_calling/all_peaks\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n\n    # run command\n    macs3 callpeak -f BAMPE -t $path_bams/${sample_name}.qc_bl_filt.sorted.bam -g mm -q 0.01 --name ${sample_name}_all --outdir $path_peaks/all_peaks/all_peaks_${sample_name}/ 2&gt; $path_peaks/all_peaks/${sample_name}_macs3.log\ndone\n</code></pre>"},{"location":"days/03_peak_calling/#22-hmmratac-peak-calling","title":"2.2 HMMRATAC Peak Calling","text":"<p>HMMRATAC is specifically designed for ATAC-seq data and uses a Hidden Markov Model to identify accessible chromatin regions. It models the distribution of fragment lengths to distinguish between nucleosome-free regions and nucleosome-bound regions, to provide a more accurate identification of open chromatin regions.  </p> <p>We will use the filtered BAM files (all fragments) for this analysis.  </p> <p>Bonus Task</p> <ul> <li>Create a new directory named: <code>results/03_peak_calling/hmmratac_peaks</code>.</li> <li>Have a look at the HMMRATAC documentation for more details on the parameters used.</li> <li>This step can take a bit longer, while waiting you can move to the next section of the tutorial.</li> </ul> Solution <p><pre><code>mkdir -p results/03_peak_calling/hmmratac_peaks\n\nfor sample_name in \"${samples[@]}\"; do\n    echo \"Processing sample: $sample_name\"\n    macs3 hmmratac -i $path_bams/${sample_name}.qc_bl_filt.sorted.bam -f BAMPE --name ${sample_name}_hmmratac --outdir $path_peaks/hmmratac_peaks/hmmratac_peaks_${sample_name}/\ndone\n</code></pre> Parameters explanation: <code>-i</code>: input file (BAM file) <code>-f BAMPE</code>: input file format is BAM paired-end <code>--outdir</code>: output directory for peak files  </p>"},{"location":"days/03_peak_calling/#3-compare-peak-calling-results","title":"3. Compare Peak Calling Results","text":"<p>Now let\u2019s visualise and compare the different peak calling results using the Integrative Genomics Viewer (IGV) to understand how each method performs.</p> <p>Task 5: Load traks into IGV </p> <p>For Cerebrum_rep1 sample, download and load the following files in IGV:</p>"},{"location":"days/03_peak_calling/#peak-files","title":"Peak Files:","text":"<ul> <li>MACS3 NF peaks: <code>results/03_peak_calling/NF_peaks/NF_peaks_Cerebrum_rep1/Cerebrum_rep1_NF_peaks.narrowPeak</code></li> <li>MACS3 all peaks: <code>results/03_peak_calling/all_peaks/all_peaks_Cerebrum_rep1/Cerebrum_rep1_all_peaks.narrowPeak</code></li> <li>HMMRATAC peaks: <code>results/03_peak_calling/hmmratac_peaks/hmmratac_peaks_Cerebrum_rep1/Cerebrum_rep1_hmmratac_accessible_regions.narrowPeak</code> (if you completed the bonus task, if you didn\u2019t and you are curious, you will find the files in <code>/data/Solutions/</code>)</li> </ul>"},{"location":"days/03_peak_calling/#bam-files-for-read-coverage","title":"BAM Files for Read Coverage:","text":"<ul> <li>All fragments BAM: <code>results/01_filtered_bams/Cerebrum_rep1.qc_bl_filt.sorted.bam</code></li> <li>Nucleosome-free fragments BAM: <code>results/01_filtered_bams/NF_bams/Cerebrum_rep1_NF.sorted.bam</code></li> </ul> <p>Task 6: Compare results </p> <ul> <li>Do you observe big differences between methods?  </li> <li>Which method would work better to study TF binding sites? and for DA analysis?   </li> <li>Can you find an example of a gene where the nucleosomal pattern at TSS can be percieved?  </li> </ul>"},{"location":"days/04_build_consensus_peak_set/","title":"Build Consensus Peak Set","text":""},{"location":"days/04_build_consensus_peak_set/#overview","title":"Overview","text":"<p>After calling peaks in individual replicates, we need to create a unified peak set for downstream analysis by identifying reproducible peaks across replicates.</p>"},{"location":"days/04_build_consensus_peak_set/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the importance of reproducible peaks in ATAC-seq analysis</li> <li>Use bedtools to intersect peaks between replicates</li> <li>Create a consensus peak set for differential accessibility analysis</li> <li>Visualize consensus peaks in IGV</li> </ul>"},{"location":"days/04_build_consensus_peak_set/#1-build-consensus-peak-annotation","title":"1. Build Consensus Peak Annotation","text":"<p>For robust downstream analysis, we need peaks that are reproducible across biological replicates. This ensures our analysis focuses on genuine accessible chromatin regions rather than technical artifacts.</p> <p>Build a consensus peak set, which will become your reference annotation for counting reads in peaks later on.</p> <p>Task 1: intersect peaks between replicates</p> <p>Find peaks that are present in both replicates within each condition:</p> <ul> <li>Create output directory: <code>results/04_consensus_peaks</code></li> <li>Use bedtools intersect with 25% reciprocal overlap requirement</li> <li>Generate separate intersection files for each condition</li> <li>In order to get original peaks from both replicates (without appending the output), run bedtools intersect twice, swapping the order to capture all peaks. </li> </ul> Hint <p>Use <code>bedtools intersect</code> with parameters: - <code>-f 0.25</code>: minimum 25% overlap - <code>-r</code>: reciprocal overlap required - <code>-wa</code>: write original entries from file A - Run twice with swapped files to capture all overlapping peaks</p> Solution <pre><code># Create output directory\nmkdir -p results/04_consensus_peaks\npath_peaks=\"results/03_peak_calling/all_peaks\"\n\n# Intersect Kidney replicates (run twice to get all overlaps)\nbedtools intersect -wa -a $path_peaks/all_peaks_Kidney_rep1/Kidney_rep1_all_peaks.narrowPeak \\\n                      -b $path_peaks/all_peaks_Kidney_rep2/Kidney_rep2_all_peaks.narrowPeak \\\n                      -f 0.25 -r &gt; results/04_consensus_peaks/Kidney_intersect.bed\n\nbedtools intersect -wa -b $path_peaks/all_peaks_Kidney_rep1/Kidney_rep1_all_peaks.narrowPeak \\\n                      -a $path_peaks/all_peaks_Kidney_rep2/Kidney_rep2_all_peaks.narrowPeak \\\n                      -f 0.25 -r &gt;&gt; results/04_consensus_peaks/Kidney_intersect.bed\n\n# Intersect Cerebrum replicates (run twice to get all overlaps)\nbedtools intersect -wa -a $path_peaks/all_peaks_Cerebrum_rep1/Cerebrum_rep1_all_peaks.narrowPeak \\\n                      -b $path_peaks/all_peaks_Cerebrum_rep2/Cerebrum_rep2_all_peaks.narrowPeak \\\n                      -f 0.25 -r &gt; results/04_consensus_peaks/Cerebrum_intersect.bed\n\nbedtools intersect -wa -b $path_peaks/all_peaks_Cerebrum_rep1/Cerebrum_rep1_all_peaks.narrowPeak \\\n                      -a $path_peaks/all_peaks_Cerebrum_rep2/Cerebrum_rep2_all_peaks.narrowPeak \\\n                      -f 0.25 -r &gt;&gt; results/04_consensus_peaks/Cerebrum_intersect.bed\n\n# Check file format\nhead results/04_consensus_peaks/Cerebrum_intersect.bed\n</code></pre> <p>Parameter explanations: - <code>-a</code>: First input file (bed or narrowPeak format) - <code>-b</code>: Second input file (bed or narrowPeak format)   - <code>-f 0.25</code>: Minimum overlap required as a fraction of A - <code>-r</code>: Require reciprocal overlap - <code>-wa</code>: Write the original entry in A for each overlap - Why twice?: Running bedtools intersect twice with swapped files ensures we capture all overlapping peaks  </p> <p>Task 2: Create final consensus peak set</p> <p>Merge the intersected peaks from both conditions into a final consensus peak set, allowing a maximum distance of 10bp between peaks to be merged (-d 10).  </p> <ul> <li>Combine peaks from both conditions</li> <li>Sort by genomic coordinates</li> <li>Merge nearby peaks (within 10bp) to avoid redundancy</li> </ul> <pre><code># concatenate intersection of both replicates and sort the file\ncat results/04_consensus_peaks/Kidney_intersect.bed results/04_consensus_peaks/Cerebrum_intersect.bed | sort -k1,1 -k2,2n &gt; results/04_consensus_peaks/consensus_peaks_temp.bed\nsort -k1,1 -k2,2n results/04_consensus_peaks/consensus_peaks_temp.bed &gt; results/04_consensus_peaks/consensus_peaks_temp_sorted.bed\n\n# Use bedtools merge to merfe peaks at shorter distance than 10bp\nbedtools merge -d 10 -i results/04_consensus_peaks/consensus_peaks_temp.bed &gt; results/04_consensus_peaks/consensus_peaks.bed\n</code></pre> <p>Add the <code>results/04_consensus_peaks/consensus_peaks.bed</code> track to IGV and have a look to the resulting peak annotation. </p> <p>Task 3: Visualize Consensus Peaks</p> <ul> <li>Load <code>results/04_consensus_peaks/consensus_peaks.bed</code> in IGV</li> <li>Compare with individual replicate peak files (for that, load results from peak calling from each sample)</li> <li>Assess the quality and coverage of your consensus peak set</li> </ul> <ol> <li>Do the consensus peaks cover the main accessible regions you observed in individual replicates?</li> <li>Was the merging of nearby regions to stringent? too relax? </li> <li>Are there regions where you lost peaks due to the stringent overlap requirements?</li> </ol> <p>Task 4: Count number of peaks</p> <ul> <li>Count how many peaks contains your consensus peak set.</li> <li>Count how many peaks each individual sample contained</li> </ul> <pre><code># Count final consensus peaks\nwc -l results/04_consensus_peaks/consensus_peaks.bed\n\n# Count peaks in each sample\nsamples=(Kidney_rep1 Kidney_rep2 Cerebrum_rep1 Cerebrum_rep2)\necho \"Number of peaks called by MACS3:\"\nfor sample_name in \"${samples[@]}\"; do\n    wc -l results/03_peak_calling/all_peaks/all_peaks_${sample_name}/${sample_name}_all_peaks.narrowPeak\ndone\n\n# Check file format\nhead results/04_consensus_peaks/consensus_peaks.bed\n</code></pre>"},{"location":"days/05_quantify_peaks/","title":"Quantify Peak Accessibility","text":""},{"location":"days/05_quantify_peaks/#overview","title":"Overview","text":"<p>After building a consensus peak set, we need to quantify the number of reads overlapping each peak in each sample. This creates a count matrix for downstream differential accessibility analysis.</p>"},{"location":"days/05_quantify_peaks/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Convert peak coordinates to appropriate format for read counting</li> <li>Use featureCounts to quantify reads in peaks across samples</li> <li>Generate comprehensive quality control reports</li> </ul>"},{"location":"days/05_quantify_peaks/#1-count-reads-on-peaks","title":"1. Count reads on peaks","text":"<p>After defining a consensus peak set, we will count the number of reads that overlap each peak in each sample. This step can be seen as counting reads in annotated genes for RNA-seq data, where instead of genes, we are counting reads in peaks.</p> <p>There are different tools that can be used for this purpose, such as featureCounts from the Subread package or bedtools coverage.  </p> <p>In this example, we will use <code>featureCounts</code>, which is a widely used tool for counting reads in genomic features.  </p>"},{"location":"days/05_quantify_peaks/#11-prepare-peak-annotation-for-quantification","title":"1.1 Prepare Peak Annotation for Quantification","text":"<p>FeatureCounts requires annotation in GTF or SAF (Simplified Annotation Format). The SAF format is a tab-delimited file with the following columns:  <code>GeneID, Chr, Start, End, Strand</code></p> <p>However, your consensus peak is in BED format with only 3 columns:  </p> <pre><code># Check file format\nhead results/04_consensus_peaks/consensus_peaks.bed\n</code></pre> <p>Task1: Compare BED to SAF format</p> <ul> <li>Which are the 2 columns you are missing to convert your BED file into SAF format? </li> <li>Can you think of a way to provide values to those fields?</li> </ul> Answer <p><code>GeneID</code> and <code>Strand</code> fields are missing. Strand is missing since the dataset has no strand information. GeneID would correspond to Interval_ID, but this information has been lost while creating the consensus set, given that peaks have been merged.  </p> <p><code>Strand</code>: Information is not mandatory, you can provide a <code>.</code> instead. <code>GeneID</code>: You can generate sequential Interval_ID with incremental numbers (e.g., \u201cInterval_1\u201d, \u201cInterval_2\u201d).  </p> <p>Task 2: Convert Consensus Peaks to SAF Format</p> <ul> <li>Create a new folder named: <code>results/05_counts_reads</code></li> <li>Use awk or an alternative way to convert the bed file to SAF format. Don\u2019t forget to add a header \u201cGeneID    Chr Start   End Strand\u201d. </li> </ul> Hint <p>Use <code>awk</code> to: - Add sequential peak IDs (e.g., \u201cInterval_1\u201d, \u201cInterval_2\u201d). - Convert BED coordinates to SAF format. - Add strand information (use \u201c.\u201d for unstranded).  </p> Solution <p><pre><code># Create output directory\nmkdir results/05_counts_reads\n\n# Create SAF file with header\necho \"GeneID    Chr Start   End Strand\" &gt; results/04_consensus_peaks/consensus_peaks.saf\n\n# Convert BED to SAF format\nawk '{OFS = \"\\t\"} {print \"Interval_\"NR,$1,$2,$3,\".\"}' results/04_consensus_peaks/consensus_peaks.bed &gt;&gt; results/04_consensus_peaks/consensus_peaks.saf\n\n# check the new file format\nhead results/04_consensus_peaks/consensus_peaks.saf\n</code></pre> What this does:. - <code>echo -e</code>: Creates header with tab separators. - <code>awk OFS=\"\\t\"</code>: Sets output field separator to tab. - <code>\"Interval_\"NR</code>: Creates unique ID using row number. - <code>$1,$2,$3</code>: BED coordinates (chr, start, end). - <code>\".\"</code>: Unstranded (appropriate for ATAC-seq peaks).  </p> <p>Now we can use featureCounts to count the reads in each peak for each sample</p> <p>Task 3: Count reads in peaks</p> <ul> <li>Run featureCounts to quantify how many filtered reads (in <code>*qc_bl_filt.sorted.bam</code>) overlap consensus peaks in each sample.  </li> <li>Use paired-end counting mode</li> <li>Count all filtered BAM files simultaneously</li> </ul> <p>Featurecounts</p> <p><code>-F SAF</code>: specify that the annotation file is in SAF format <code>-p</code>: specify that the input files are paired-end <code>-a</code>: specify the annotation file <code>-o</code>: specify the output file  </p> <p>These are only some of all parameteres one can apply. You can also adjust the parameters of featureCounts based on your specific requirements, such as setting a minimum mapping quality or handling multi-mapping reads.</p> Solution <pre><code>path_bams=\"results/01_filtered_bams\"\nfeatureCounts -F SAF -T 2 -p -a results/04_consensus_peaks/consensus_peaks.saf -o results/05_counts_reads/feature_Counts.txt $path_bams/*qc_bl_filt.sorted.bam 2&gt; results/05_counts_reads/featureCounts.log\n</code></pre> <p>The output file will contain the counts of reads in each peak for each sample, which can be used for downstream analysis such as differential accessibility analysis.</p> <p>Task 4: Examine the results</p> <p>Have a look to the output, which file contains the counts matrix?: <pre><code>ls results/05_counts_reads/\n</code></pre></p> Answer <p>FeatureCounts generates: - <code>feature_Counts.txt</code>: Main count matrix. - <code>feature_Counts.txt.summary</code>: Counting statistics. - <code>featureCounts.log</code>: Processing log and any warnings.  </p> <p>File <code>results/05_counts_reads/feature_Counts.txt</code> contains the counts table with: - Rows: Annotated peaks. - Columns: peaks annotation information (from SAF file) + Samples (counts in each sample)</p> <ul> <li>Check how many peaks contains the counts matrix, is it the same number you had in your consensus peak set?</li> </ul> <pre><code># Count number of peaks quantified\nwc -l results/05_counts_reads/feature_Counts.txt\n</code></pre> Answer <p>Yes, it is the same number of peaks, although not same number of rows because <code>feature_Counts.txt</code> has 2 lines header.</p>"},{"location":"days/05_quantify_peaks/#2-quality-control-assessment","title":"2.  Quality Control Assessment","text":"<p>After filtering, peak calling and counting reads in peaks, we can run MultiQC to aggregate the QC metrics from different steps and generate a comprehensive report.  </p> <p>This will help us to assess further the overall quality of the ATAC-seq data and identify any potential issues that may need to be addressed before proceeding with downstream analysis.  </p> <p>Task 3: Generate MultiQC Report </p> <ul> <li>Run multiqc on the entire <code>results</code> directory</li> </ul> Solution <p><pre><code>multiqc --outdir results/multiQC_report --title \"ATAC-seq_Pipeline_Summary\" results/\n</code></pre> Parameter explanations:. - <code>--outdir</code>: Output directory for the report. - <code>--title</code>: Custom title for the report. - <code>results/</code>: Search this directory for QC files.  </p> <p>Download and open the report and explore the QC metrics:</p> <ul> <li>How many peaks were successfully quantified?   </li> <li>What percentage of reads were assigned to peaks in each sample?   </li> <li>Are there differences in counting efficiency between samples?   </li> <li>Do any samples show concerning quality metrics?   </li> </ul>"},{"location":"days/06_differential_accessibility_analysis/","title":"Differential Accessibility Analysis","text":""},{"location":"days/06_differential_accessibility_analysis/#overview","title":"Overview","text":"<p>After quantifying reads in consensus peaks, we will identify regions with significantly different accessibility between conditions using DESeq2.</p>"},{"location":"days/06_differential_accessibility_analysis/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Import and prepare count data for differential analysis</li> <li>Create proper metadata for experimental design</li> <li>Perform differential accessibility analysis with DESeq2</li> <li>Visualize and interpret results</li> <li>Prepare data for downstream functional analysis</li> </ul> <p>Note</p> <p>For this section, you have to move to R. Change from <code>Terminal</code> to <code>Console</code> in your Rstudio server. To do that, click on the <code>Console</code> tab on the top left of your terminal. </p> <p>Load necessary packages for the analysis     <pre><code>library(DESeq2)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(GenomicRanges)\n</code></pre></p>"},{"location":"days/06_differential_accessibility_analysis/#1-import-read-counts-and-metadata","title":"1. Import read counts and metadata","text":"<p>Start by loading featureCounts output as a matrix of counts</p> <p>Task 1: Load counts matrix</p> <ul> <li>Read the output of featurecounts that contains counts for all peaks in all samples: <code>results/05_counts_reads/feature_Counts.txt</code></li> <li>Keep the header of the table</li> <li>Process the table in order to keep: read-counts columns only, peak IDs as row.names, simple column names (sample names)</li> <li>Convert the table into matrix format</li> </ul> Solution <pre><code># Read table of counts (output from FeatureCounts)\ncounts_file &lt;- \"results/05_counts_reads/feature_Counts.txt\"\ncts_table &lt;- read.table(counts_file, header = T)\n\n# convert into matrix, with interval IDs as rownames\ncts &lt;- cts_table[,7:10]\nrow.names(cts) &lt;- cts_table[,1]\ncolnames(cts) &lt;- gsub(colnames(cts), pattern = \"results.01_filtered_bams.\", replacement = \"\") %&gt;% \n                 gsub(., pattern=\".qc_bl_filt.sorted.bam\", replacement=\"\")\n#check the object \ncts\n</code></pre> <p>Task 2: Create sample metadata</p> <ul> <li>Create metadata table as data.frame</li> <li>Each row is a sample, columns contain metadata information about samples (e.g. condition/treatment/batch\u2026). Sample names are row.names</li> <li>Here you only have information about condition: Kidney and Cerebrum. The metadata will contain only one column. If you would have a second factor, e.g. treatment (treated vs untreated), you would need to add this information as a second column in the data.frame.  </li> <li>Use factor variables (not character variables)</li> </ul> <p>Warning</p> <p>Important: The order of the rows in the data.frame must match the order of the columns in the counts matrix            The metadata data.frame must have factor variables (not character variables)</p> <pre><code>condition &lt;- factor( c(rep(\"Cerebrum\",2), rep(\"Kidney\",2)) )\ncolData &lt;- data.frame(condition, row.names = colnames(cts))\n\n# Verify the setup\nprint(colData)\nall(rownames(colData) == colnames(cts))\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/#2-deseq2-differential-analysis","title":"2. DESeq2 Differential Analysis","text":"<p>Bring together counts and metadata to create a DESeq object</p> <pre><code>dds &lt;- DESeqDataSetFromMatrix(\n  countData = cts, colData = colData, \n  design = ~ condition)\ndim(dds)\n</code></pre> <p>Optional: (Remove peaks with insufficient read counts)</p> <pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 30) &gt;= 2\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <p>Perform the estimation of dispersions</p> <pre><code>dds &lt;- DESeq(dds)\n</code></pre> <p>And plot PCA of the samples <pre><code>vsd &lt;- varianceStabilizingTransformation(dds, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"condition\"), ntop=\"all\")\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\nplotPCA(vsd, intgroup=c(\"sizeFactor\"), ntop=\"all\")\n</code></pre></p>"},{"location":"days/06_differential_accessibility_analysis/#3-explore-da-results","title":"3. Explore DA results.","text":"<p>After the dispersion estimates have been calculated, you can proceed to test for differential accessibile (DA) regions between the two conditions (Kidney vs Cerebrum).  </p> <p>The <code>results()</code> function extracts the results table from DA analysis with the log2 fold changes, p-values and adjusted p-values for each peak.  </p> <pre><code>DA_results &lt;- results(dds)\nsummary( DA_results )\nhead(DA_results)\n</code></pre> <p>Have a look in <code>DA_results</code> object.  </p> <ul> <li>How many peaks have been analysed?</li> <li>At a cutoff of padj &lt; 0.01, how many significant DA peaks there are?</li> <li>How many have singificantly higher accessibility in Kidney compared to Cerebrum? </li> </ul> Solution <pre><code>    cat(\"Total peaks analyzed:\", nrow(DA_results), \"\\n\")\n    cat(\"Significant peaks:\", sum(DA_results$padj &lt; 0.01, na.rm = TRUE), \"\\n\")\n    cat(\"Upregulated in Kidney:\", sum(DA_results$log2FoldChange &gt; 0 &amp; \n                                    DA_results$padj &lt; 0.01, na.rm = TRUE), \"\\n\")\n    cat(\"Upregulated in Cerebrum:\", sum(DA_results$log2FoldChange &lt; 0 &amp; \n                                    DA_results$padj &lt; 0.01, na.rm = TRUE), \"\\n\")\n</code></pre> <p>You can visualise the results of DA peaks with a Volcano plot, analogous to RNAseq DE genes results</p> <pre><code> # Create volcano plot\n    volcano_data &lt;- as.data.frame(DA_results)\n    volcano_data$significant &lt;- abs(volcano_data$log2FoldChange) &gt; 2 &amp; volcano_data$padj &lt; 0.01\n\n    ggplot(volcano_data, aes(x = log2FoldChange, y = -log10(padj))) +\n      geom_point(aes(color = significant), alpha = 0.6) +\n      scale_color_manual(values = c(\"FALSE\" = \"grey\", \"TRUE\" = \"red\")) +\n      geom_vline(xintercept = c(-2, 2), linetype = \"dashed\", alpha = 0.5) +\n      geom_hline(yintercept = -log10(0.01), linetype = \"dashed\", alpha = 0.5) +\n      labs(x = \"Log2 Fold Change\", \n           y = \"-Log10 Adjusted P-value\",\n           title = \"Volcano Plot: Kidney vs Cerebrum\",\n           subtitle = \"Red points: |Log2FC| &gt; 2 and padj &lt; 0.01\") +\n      theme_minimal()\n</code></pre> <p>Note</p> <p>By default, the results() function will extract the results for the last variable in the design formula (here: condition) and will perform a comparison of the second level of the factor over the first level (here: Kidney over Cerebrum). If you want to extract results for a different comparison, you can specify the contrast argument. Have a look at DESeq2 documentation for more information.</p>"},{"location":"days/06_differential_accessibility_analysis/#4-save-differential-accessibility-results-for-downstream-analysis","title":"4. Save differential accessibility results for downstream analysis:","text":"<p>Save differential accessibility results for downstream analysis:</p> <pre><code>dir.create(\"results/06_DA_analysis\")\nwrite.table(DA_results, file=\"results/06_DA_analysis/DA_results.txt\", quote=FALSE)\n</code></pre>"},{"location":"days/06_differential_accessibility_analysis/#5-create-granges-object","title":"5. Create GRanges object","text":"<p>Convert the peak coordinates into a GRanges object (from GenomicRanges package). <code>GRanges</code> class represents a collection of genomic ranges and associated data to them.  In this case, you will use it to represent the peak coordinates, and add as metadata the log2 fold changes and adjusted p-values from the differential accessibility analysis.</p> <p><pre><code>    # Prepare peak coordinates object \n    peaks_coord &lt;- cts_table[,1:4]\n    head(peaks_coord)\n\n    # Select information from DESeq2 you want to keep\n    DA_stats &lt;- as.data.frame(DA_results)[c(2,3,5,6)]\n    head(DA_stats)\n\n    # Merge both objects and convert them into GRanges class\n    peaks_df &lt;- merge(DA_stats, peaks_coord, by.x=\"row.names\", by.y=\"Geneid\")\n    peaks_df$Chr &lt;- paste0(\"chr\",peaks_df$Chr)\n    peaks_gr = makeGRangesFromDataFrame(peaks_df, keep.extra.columns=T)\n\n    # Have a look into the new object\n    head(peaks_gr)\n</code></pre> To this object <code>peaks_gr</code>, you will add other metadata along the downstream analysis, like genomic overlap.</p> <p>Save the object <pre><code>saveRDS(peaks_gr, \"results/06_DA_analysis/DA_results.RDS\")\n</code></pre></p> <p>And clean the environment</p> <p>Warning</p> <p>You must save the objects before running the following code. After running it your variables and objects will be removed.</p> <pre><code>rm(list = ls())\n.rs.restartR()\n</code></pre>"},{"location":"days/07_annotate_peaks/","title":"Peak Annotation and Functional Analysis","text":"<p>After performing differential accessibility analysis with DESeq2, you will annotate peaks based on their genomic location using ChIPseeker and perform functional enrichment analysis.</p>"},{"location":"days/07_annotate_peaks/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Load differential accessibility results and prepare for annotation</li> <li>Understand genomic feature annotation using ChIPseeker</li> <li>Visualize peak distribution and genomic context</li> <li>Perform functional enrichment analysis on peak-associated genes </li> <li>Interpret biological significance of accessibility changes </li> </ul> <p>Load necessary packages for the analysis </p> <pre><code># Load libraries\nlibrary(ChIPseeker)\nrequire(\"TxDb.Mmusculus.UCSC.mm10.knownGene\")\nlibrary(clusterProfiler)\nlibrary(\"org.Mm.eg.db\")\n\n\n# Set up annotation databases\nTxDb &lt;- TxDb.Mmusculus.UCSC.mm10.knownGene\nAnnoDb &lt;- 'org.Mm.eg.db'\n</code></pre>"},{"location":"days/07_annotate_peaks/#1-load-da-results","title":"1. Load DA results","text":"<p>Load the DA results you have generated in the previous section, and name it <code>DA_results</code> again:</p> Solution <pre><code>gr &lt;- readRDS(\"results/06_DA_analysis/DA_results.RDS\")\nhead(gr)\n</code></pre>"},{"location":"days/07_annotate_peaks/#2-visualize-peak-distribution","title":"2. Visualize Peak Distribution","text":"<p>Let\u2019s visualize how peaks are distributed across the genome. In this case, they are all located at chr6, but when analysing the entire genome you may want to see if they cluster in specific chromosomes or locations.  <code>covplot</code> function from ChipSeeker package allows you to do that: </p> <p>Task1: Plot peaks across the genome</p> <ul> <li>Serach for <code>covplot</code> function in ChipSeeker documentation  and plot the genomic location of peaks, as well as their logFC. </li> </ul> Solution <pre><code># Create coverage plot showing peak positions and their fold changes\ncovplot(gr, weightCol = \"log2FoldChange\")\n</code></pre> <p>This plot shows the distribution of peaks across chromosomes, with y-axis representing the log2 fold change values.</p>"},{"location":"days/07_annotate_peaks/#3-filter-significant-peaks","title":"3. Filter Significant Peaks","text":"<p>Split peaks into upregulated and downregulated categories:</p> <ul> <li>Sort GRanges by genomic coordinates </li> <li>Filter for significant peaks (padj &lt; 0.01)</li> <li>Apply fold change thresholds (|log2FC| &gt; 2)</li> </ul> <pre><code># Sort the GRanges object by genomic coordinates\ngr &lt;- sort(gr, by = ~ seqnames + start + end)\n\n# Split peaks into upregulated and downregulated based on significance and fold change\ngr_list &lt;- list(\n  up = gr[gr$padj &lt; 0.01 &amp; gr$log2FoldChange &gt; 2,], \n  down = gr[gr$padj &lt; 0.01 &amp; gr$log2FoldChange &lt; -2,]\n)\n\n# Check the structure of gr_lists (list with 2 GRanges object)\nhead(gr_list)\n</code></pre> <p>Task 2: Explore gr objects </p> <ul> <li>Check how many rows contain each object of the list, are the numbers balanced?</li> </ul> Solution <pre><code># Count peaks in each category\ncat(\"Number of upregulated peaks:\", length(gr_list$up), \"\\n\")\ncat(\"Number of downregulated peaks:\", length(gr_list$down), \"\\n\")\n</code></pre> <ul> <li>Which tissue shows more significantly accessible regions?</li> <li>Is this in agreement with what you have seen in the Volcano plot?</li> </ul>"},{"location":"days/07_annotate_peaks/#4-annotate-genomic-overlap-with-chipseeker","title":"4. Annotate Genomic Overlap with ChIPseeker","text":"<p>Next, you will annotate peaks to determine their genomic context (promoters, exons, introns, etc.):</p> <p>Annotation Parameters:. - TSS region: \u00b11000 bp around transcription start sites. - Include detailed genomic annotations. - Separate analysis for up and down regulated peaks. </p> <pre><code># Annotate all peaks\npeakAnno = annotatePeak(gr, \n                        tssRegion=c(-1000, 1000), \n                        TxDb=TxDb, \n                        annoDb=AnnoDb, \n                        overlap = \"TSS\")\n\n\n# Annotate upregulated peaks\npeakAnno_up &lt;- annotatePeak(gr_list$up, \n                           tssRegion = c(-1000, 1000), \n                           TxDb = TxDb, \n                           annoDb = AnnoDb, \n                           overlap = \"TSS\")\n\n# Annotate downregulated peaks\npeakAnno_down &lt;- annotatePeak(gr_list$down, \n                             tssRegion = c(-1000, 1000), \n                             TxDb = TxDb, \n                             annoDb = AnnoDb, \n                             overlap = \"TSS\")\n\n\n# Save the objects\nsaveRDS(peakAnno, \"results/06_DA_analysis/Annotated_peaks.rds\")\nsaveRDS(peakAnno_up, \"results/06_DA_analysis/Annotated_peaks_up.rds\")\nsaveRDS(peakAnno_down, \"results/06_DA_analysis/Annotated_peaks_down.rds\")\n</code></pre> <p>Task 3: Examine Annotation Results</p> <p>Explore the new objects created after annotation.   </p> <ul> <li>What is the frequency of promoters overlap overall?</li> <li>Do upregulated and downregulated peaks have different frequencies?  </li> </ul> <p><code>peakAnno</code>, you can find different layers of information if you do <code>peakAnno@&lt;tab&gt;</code>:</p> <pre><code># Look at all peaks annotation summary\npeakAnno\n# Look at the upregulated peaks annotation summary\npeakAnno_up\n# Look at the downregulated peaks annotation summary\npeakAnno_down\n</code></pre> <p>You can examine <code>peakAnno</code> object by doing <code>peakAnno@&lt;tab&gt;</code>, you will find several layers of information.  </p> <ul> <li>How many downregulated peaks overlap exons?  </li> </ul> <pre><code># Count how many downregulated peaks overlap exonic regions\nsum(peakAnno_down@detailGenomicAnnotation$Exon)\n</code></pre> <p>Task 4: Visualise Peak Annotations</p> <p>Use <code>plotAnnoPie</code> function from ChipSeeker package to visualize the genomic overlap distribution of upregulated peaks and downregulated peaks, separately:</p> Solution <pre><code># Plot pie charts for peak categories\nplotAnnoPie(peakAnno_up, main = \"\\n\\nDA up\") \nplotAnnoPie(peakAnno_down, main = \"\\n\\nDA down\")\n</code></pre> <p>These plots show the percentage of peaks falling into different genomic categories (promoters, exons, introns, intergenic regions, etc.).</p>"},{"location":"days/07_annotate_peaks/#5-functional-enrichment-analysis","title":"5. Functional Enrichment Analysis","text":"<p>Next, we will focus in peaks overlaping promoter regions (TSS) of genes, and we will perform functional enrichment analysis on those genes to understand which biological processes or metabolic pathways may be afected by the changes in chromatin accessibility.</p> <p>Task 6: Prepare gene list</p> <p>Start testing DA peaks that are upregulated (higher accessibility in Kidney).  </p> <ul> <li>Take DA upregulated peaks (peakAnno_up), select those overlapping TSS (Promoter), and get the list of gene SYMBOLs of the overlapping genes.  </li> <li>Get a list of universe gene SYMBOLs to compare the DA upregulated genes to.  </li> </ul> Solution <pre><code># Extract gene symbols for peaks in promoter regions\ngenes_tss_up &lt;- peakAnno_up@anno[peakAnno_up@detailGenomicAnnotation$Promoter, \"SYMBOL\"]\n\n# Create universe of genes with all promoter-overlapping peaks for background\nuniverse &lt;- peakAnno@anno[peakAnno@detailGenomicAnnotation$Promoter,]$SYMBOL\n</code></pre> <p>Task 7: Gene Ontology (GO) Enrichment Analysis</p> <p>Use <code>enrichGO</code> function from <code>ClusterProfiler</code> to perform GO Enrichment Analysis of <code>genes_tss_up</code> set.   </p> <ul> <li>Provide unique universe. </li> <li>Choose Biological Process (BP) ontology.  </li> <li>Apply multiple testing correction, and pvalueCutoff 0.05</li> <li>Pass AnnoDb variable created at the beginning of this tutorial section for OrgDb.  </li> <li>Specify keyType (SYMBOL).  </li> </ul> Solution <pre><code># GO enrichment for upregulated genes\nego_up &lt;- enrichGO(gene          = genes_tss_up$SYMBOL,\n                universe      = unique(universe),\n                OrgDb         = AnnoDb,\n                keyType       = \"SYMBOL\",\n                ont           = \"BP\",  \n                pAdjustMethod = \"BH\",\n                pvalueCutoff  = 0.05,\n                qvalueCutoff  = 0.05,\n                readable      = TRUE)\n\n# Check results\nego_up\n</code></pre> <p>Task 8 </p> <ul> <li> <p>Did you find significantly enriched GO terms?  </p> </li> <li> <p>Try to run the same analysis considering all genes as a universe, why do you think there is a difference?  </p> </li> <li> <p>Visualise the GO terms using a function from ClusterProfiler such as: <code>barplot()</code></p> </li> </ul> Solution <pre><code>barplot(ego_up, showCategory = 20)\n</code></pre> <p>Bonus</p> <p>Repeat tasts 6-8 for DA downregulated peaks (higher accessiblility in Cerebrum)</p> Solution <pre><code># Create list of gene SYMBOLS associated to TSS overlapping peaks\ngenes_tss_down &lt;- peakAnno_down@anno[peakAnno_down@detailGenomicAnnotation$Promoter, \"SYMBOL\"]\n\n# Run enrichGO using gene SYMBOLS associated to all peaks overlapping promoters\nego_down &lt;- enrichGO(gene   = genes_tss_down$SYMBOL,\n            universe      = unique(universe),\n            OrgDb         = AnnoDb,\n            keyType       = \"SYMBOL\",\n            ont           = \"BP\",  \n            pAdjustMethod = \"BH\",\n            pvalueCutoff  = 0.05,\n            qvalueCutoff  = 0.05,\n            readable      = TRUE)\n\n# Plot results\nbarplot(ego_down, showCategory = 20)\n</code></pre> <p>Next Steps</p> <p>You can change the <code>ont</code> parameter to explore different ontologies: - \u201cMF\u201d: Molecular Function. - \u201cBP\u201d: Biological Process  - \u201cCC\u201d: Cellular Component  </p>"},{"location":"days/07_annotate_peaks/#7-additional-visualizations","title":"7. Additional Visualizations","text":"<p>There are several other plots that ChipSeeker allowes you to do.  You can explore a couple of examples related to plotting the profile of peaks around certain regions</p> <p>Task 9: Peak Heatmap</p> <ul> <li>Create a heatmap showing peak signal around TSS:  </li> </ul> <pre><code># Generate peak heatmap around gene bodies\npeakHeatmap(peak = gr,\n            TxDb = TxDb,\n            upstream = 1000,\n            downstream = 1000,\n            by = \"gene\",\n            type = \"start_site\",\n            nbin = 800)\n</code></pre> <p>Bonus Task</p> <p>Now let\u2019s plot the profile of peak counts around annotated TSS split by conditions</p> <pre><code># Get TSS (promoter) annotation for mouse mm10\npromoter &lt;- getPromoters(TxDb=TxDb, upstream=1000, downstream=1000)\n\n# Build a matrix of peaks per conditions, and combine them in a list\ntagMatrix_Cer &lt;- getTagMatrix(gr_list$down , windows=promoter)\ntagMatrix_Kid &lt;- getTagMatrix(gr_list$up , windows=promoter)\ntagMatrixList &lt;- list(Cerebrum=agMatrix_Cer , Kidney=tagMatrix_Kid )\n\n# plot \nplotAvgProf(tagMatrixList, xlim=c(-1000, 1000), conf=0.95,resample=500, facet=\"row\")\n</code></pre>"}]}